{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ae9689-ef9f-4094-ba77-c72c499676ae",
   "metadata": {},
   "source": [
    "Setup & imports\n",
    "\n",
    "In this section we import all required libraries and initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_openai_client():\n",
    "    key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not key:\n",
    "        key = input(\"Enter your OpenAI API key: \").strip()\n",
    "        os.environ[\"OPENAI_API_KEY\"] = key\n",
    "\n",
    "    return OpenAI(api_key=key)\n",
    "\n",
    "client = get_openai_client()\n",
    "print(\"OpenAI client is ready.\")\n",
    "\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
    "   os.environ[\"OPENAI_API_KEY\"] = input(\"OPENAI-API-KEY\")\n",
    "print(\"API key OK.\")\n",
    "print(os.environ.get(\"OPENAI_API_KEY\") is not None)\n",
    "\n",
    "# create results folders relative to notebooks/\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "os.makedirs(\"../results/confusion_matrices\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b80b9-c32a-4f0c-911b-d41151f10078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS AND OPENAI CLIENT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# expects that OPENAI_API_KEY is set in the environment, e.g. via .env or system env\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR-API-KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "DATA_DIR, list(DATA_DIR.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad503af-99ac-45ca-be21-da8e19c87310",
   "metadata": {},
   "source": [
    "1. Load & preprocess\n",
    "\n",
    "We load the dataset, merge headline and body text, clean missing values, and perform a basic exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f92a6e-a46e-44da-8f34-bcb76aade05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / \"data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b9eae-b83f-474b-ad79-ee348ebfd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS TEXT\n",
    "\n",
    "df[\"text\"] = df[\"Headline\"].fillna(\"\") + \" \" + df[\"Body\"].fillna(\"\")\n",
    "data = df[[\"text\", \"Label\"]].rename(columns={\"Label\": \"label\"})\n",
    "data = data.dropna(subset=[\"text\", \"label\"])\n",
    "\n",
    "data.head()\n",
    "\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50343d3-2889-4df1-a51a-c9ceb3e2ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=\"label\", data=data)\n",
    "plt.title(\"Distribution of Real vs Fake News\")\n",
    "plt.xlabel(\"Label (0 = Real, 1 = Fake)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "data[\"text_length\"] = data[\"text\"].apply(lambda x: len(x.split()))\n",
    "data[\"text_length\"].describe()\n",
    "\n",
    "# HISTOGRAM\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(data[\"text_length\"], bins=50)\n",
    "plt.title(\"Distribution of Text Lengths\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "data.groupby(\"label\")[\"text_length\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac647c-bb89-44ad-8205-46c669f24b0d",
   "metadata": {},
   "source": [
    "2. Train / Test Split\n",
    "\n",
    "In this section, we split the cleaned dataset into training and test sets while preserving the original class distribution (stratification).\n",
    "This ensures that both the training and evaluation phases maintain a balanced representation of real (0) and fake (1) news examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a43b6f-1d0f-47ac-a185-d66fc8457f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN / TEST SPLIT\n",
    "\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c96fc",
   "metadata": {},
   "source": [
    "3. Sample Extraction (80 Examples from the Test Set)\n",
    "\n",
    "Here we extract a small, representative sample of 80 examples from the test set.\n",
    "This subset is used for:\n",
    "\n",
    "comparing the ML model against LLM-based models\n",
    "\n",
    "faster LLM evaluation (to reduce inference cost and time)\n",
    "\n",
    "analyzing specific cases where models disagree\n",
    "\n",
    "The sample consists of sample_texts (input articles) and sample_labels (ground truth labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343efc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE (80 exampel from the test group)\n",
    "\n",
    "SAMPLE_SIZE = 80\n",
    "\n",
    "X_test_sample = X_test[:SAMPLE_SIZE]\n",
    "y_test_sample = y_test[:SAMPLE_SIZE]\n",
    "\n",
    "sample_texts = X_test_sample.tolist()\n",
    "sample_labels = y_test_sample.to_numpy()\n",
    "\n",
    "len(sample_texts), len(sample_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4539ed",
   "metadata": {},
   "source": [
    "4. TF-IDF + Logistic Regression Pipeline\n",
    "\n",
    "This section implements the traditional Machine Learning baseline for fake/real news classification:\n",
    "\n",
    "TF–IDF vectorization of the article text\n",
    "\n",
    "Logistic Regression classifier\n",
    "\n",
    "Training on the training split\n",
    "\n",
    "Evaluation on the test split\n",
    "\n",
    "This model serves as the baseline for comparison with GPT-based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216bab0-94b9-466d-9c3e-d975eec0e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + LOGISTIC REGRESSION PIPELINE\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        stop_words=\"english\"\n",
    "    )),\n",
    "    (\"lr\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "ml_preds = model.predict(X_test)\n",
    "print(\"Logistic Regression accuracy:\", accuracy_score(y_test, ml_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5e38a",
   "metadata": {},
   "source": [
    "5. ML Model Evaluation\n",
    "\n",
    "We evaluate the Logistic Regression model using:\n",
    "\n",
    "accuracy\n",
    "\n",
    "per-class precision, recall, and F1-score\n",
    "\n",
    "This establishes a performance reference point before testing the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778deb96-d913-4f25-a3b3-e691a227bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION OF ML MODEL\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17966ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI CLIENT SETUP\n",
    "client = OpenAI()  # expects your OPENAI_API_KEY in environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d06c78",
   "metadata": {},
   "source": [
    "6. classify_with_llm() — LLM Classification Function\n",
    "\n",
    "This helper function sends a binary classification request to an LLM (GPT-4o-mini or GPT-4o).\n",
    "It:\n",
    "\n",
    "formats the prompt\n",
    "\n",
    "requests a single prediction (“0” for REAL, “1” for FAKE)\n",
    "\n",
    "parses the LLM output\n",
    "\n",
    "handles API errors gracefully so notebook execution does not break\n",
    "\n",
    "The function returns an integer prediction: 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_llm(model_name, text):\n",
    "    \"\"\"\n",
    "    Sends a zero-shot classification request to an LLM.\n",
    "    Expected return: 0 or 1\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following news article as REAL (0) or FAKE (1).\n",
    "    Answer with only 0 or 1.\n",
    "\n",
    "    Article:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1\n",
    "        )\n",
    "\n",
    "        pred = response.choices[0].message[\"content\"].strip()\n",
    "        return int(pred) if pred in [\"0\", \"1\"] else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLM error:\", e)\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "# run_llm_on_sample() — Batch Evaluation Helper\n",
    "def run_llm_on_sample(model_name: str, texts):\n",
    "    preds = []\n",
    "    for t in tqdm(texts, desc=f\"{model_name} predicting\"):\n",
    "        preds.append(classify_with_llm(model_name, t))\n",
    "    return np.array(preds, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sample of 80 examples from the test set ===\n",
    "\n",
    "SAMPLE_SIZE = 80\n",
    "\n",
    "# use the first 80 examples from X_test / y_test\n",
    "X_test_sample = X_test[:SAMPLE_SIZE]\n",
    "y_test_sample = y_test[:SAMPLE_SIZE]\n",
    "\n",
    "sample_texts = X_test_sample.tolist()\n",
    "sample_labels = y_test_sample.to_numpy()\n",
    "\n",
    "len(sample_texts), len(sample_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e08f25",
   "metadata": {},
   "source": [
    "8. GPT-4o-mini Evaluation\n",
    "\n",
    "Here we evaluate GPT-4o-mini on the 80-sample subset.\n",
    "We compute and display:\n",
    "\n",
    "accuracy\n",
    "\n",
    "classification report (precision, recall, F1)\n",
    "\n",
    "GPT-4o-mini serves as a lightweight LLM baseline — cheaper and faster than full GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_preds_mini = []\n",
    "print(\"Evaluating GPT-4o-mini...\")\n",
    "\n",
    "for text in tqdm(sample_texts):\n",
    "    llm_preds_mini.append(classify_with_llm(\"gpt-4o-mini\", text))\n",
    "\n",
    "print(\"Accuracy (GPT-4o-mini):\", accuracy_score(sample_labels, llm_preds_mini))\n",
    "print(classification_report(sample_labels, llm_preds_mini))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a15dfe",
   "metadata": {},
   "source": [
    "9. GPT-4o (Large Model) Evaluation\n",
    "\n",
    "In this section we run the same evaluation using GPT-4o (large).\n",
    "We compare:\n",
    "\n",
    "model accuracy\n",
    "\n",
    "error patterns\n",
    "\n",
    "improvements over GPT-4o-mini\n",
    "\n",
    "performance relative to the ML baseline\n",
    "\n",
    "This provides a clear picture of how model scale affects real-world classification ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44129ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_preds_4o = []\n",
    "print(\"Evaluating GPT-4o (Large)...\")\n",
    "\n",
    "for text in tqdm(sample_texts):\n",
    "    llm_preds_4o.append(classify_with_llm(\"gpt-4o\", text))\n",
    "\n",
    "print(\"Accuracy (GPT-4o):\", accuracy_score(sample_labels, llm_preds_4o))\n",
    "print(classification_report(sample_labels, llm_preds_4o))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e952e",
   "metadata": {},
   "source": [
    "10. Accuracy Comparison — ML vs. LLMs\n",
    "\n",
    "We combine the accuracy scores from all three models:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "GPT-4o-mini\n",
    "\n",
    "GPT-4o\n",
    "\n",
    "The results are visualized in a bar chart for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks all variables before plotting\n",
    "\n",
    "required_vars = [\"ml_preds\", \"llm_preds_mini\", \"llm_preds_4o\", \"y_test\", \"sample_labels\"]\n",
    "\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing variables: {missing}. Run all ML + LLM cells first.\")\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"GPT-4o-mini\", \"GPT-4o\"],\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(y_test, ml_preds),\n",
    "        accuracy_score(sample_labels, llm_preds_mini),\n",
    "        accuracy_score(sample_labels, llm_preds_4o)\n",
    "    ]\n",
    "}\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=res_df, x=\"Model\", y=\"Accuracy\")\n",
    "plt.title(\"Accuracy Comparison: ML vs LLM\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "plt.savefig(\"../results/accuracy_comparison.png\",\n",
    "            dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2ecc4",
   "metadata": {},
   "source": [
    "11. Confusion Matrix — GPT-4o-mini\n",
    "\n",
    "This section creates and visualizes the confusion matrix for GPT-4o-mini on the 80-sample evaluation set.\n",
    "It shows:\n",
    "\n",
    "how well the model distinguishes REAL vs. FAKE\n",
    "\n",
    "common misclassifications\n",
    "\n",
    "whether the model is biased toward one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f041324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFUSION MATRIX FOR GPT-4o-mini\n",
    "cm = confusion_matrix(sample_labels, llm_preds_mini)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot(cmap=\"Purples\")\n",
    "plt.title(\"Confusion Matrix — GPT-4o-mini (80 samples)\")\n",
    "plt.show()\n",
    "plt.savefig(\"../results/confusion_matrices/gpt4omini_confmat.png\",\n",
    "            dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3ba96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fake Real News (Linux)",
   "language": "python",
   "name": "fake-real-news-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
