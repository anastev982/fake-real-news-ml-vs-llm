{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ae9689-ef9f-4094-ba77-c72c499676ae",
   "metadata": {},
   "source": [
    "Setup & imports\n",
    "\n",
    "In this section we import all required libraries and initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73b80b9-c32a-4f0c-911b-d41151f10078",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND OPENAI CLIENT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# expects that OPENAI_API_KEY is set in the environment, e.g. via .env or system env\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR-API-KEY\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\".\") / \"data\"\n",
    "DATA_DIR, list(DATA_DIR.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad503af-99ac-45ca-be21-da8e19c87310",
   "metadata": {},
   "source": [
    "1. Load & preprocess\n",
    "\n",
    "We load the dataset, merge headline and body text, clean missing values, and perform a basic exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f92a6e-a46e-44da-8f34-bcb76aade05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / \"data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b9eae-b83f-474b-ad79-ee348ebfd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS TEXT\n",
    "\n",
    "df[\"text\"] = df[\"Headline\"].fillna(\"\") + \" \" + df[\"Body\"].fillna(\"\")\n",
    "data = df[[\"text\", \"Label\"]].rename(columns={\"Label\": \"label\"})\n",
    "data = data.dropna(subset=[\"text\", \"label\"])\n",
    "\n",
    "data.head()\n",
    "\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50343d3-2889-4df1-a51a-c9ceb3e2ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=\"label\", data=data)\n",
    "plt.title(\"Distribution of Real vs Fake News\")\n",
    "plt.xlabel(\"Label (0 = Real, 1 = Fake)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "data[\"text_length\"] = data[\"text\"].apply(lambda x: len(x.split()))\n",
    "data[\"text_length\"].describe()\n",
    "\n",
    "# HISTOGRAM\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(data[\"text_length\"], bins=50)\n",
    "plt.title(\"Distribution of Text Lengths\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "data.groupby(\"label\")[\"text_length\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac647c-bb89-44ad-8205-46c669f24b0d",
   "metadata": {},
   "source": [
    "2. ML baseline\n",
    "- Train / Test Split\n",
    "We split the cleaned dataset into training and test sets using stratification to preserve class balance.\n",
    "\n",
    "- Machine Learning Baseline — TF-IDF + Logistic Regression\n",
    "We build a traditional ML pipeline consisting of:\n",
    " - TF-IDF vectorizer\n",
    " - Logistic Regression classifier\n",
    "This serves as the supervised baseline.\n",
    "\n",
    "- ML Feature Interpretation — Top Predictive Words\n",
    "\n",
    "We extract the strongest positive and negative coefficients from the Logistic Regression model to understand which words push predictions toward class 0 (real) or class 1 (fake).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a43b6f-1d0f-47ac-a185-d66fc8457f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN / TEST SPLIT\n",
    "\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216bab0-94b9-466d-9c3e-d975eec0e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + LOGISTIC REGRESSION PIPELINE\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        stop_words=\"english\"\n",
    "    )),\n",
    "    (\"lr\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778deb96-d913-4f25-a3b3-e691a227bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION OF ML MODEL\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4d803-b70b-4fc8-b825-433574f8f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE (TOP WORDS FOR EACH CLASS)\n",
    "\n",
    "feature_names = model.named_steps[\"tfidf\"].get_feature_names_out()\n",
    "\n",
    "tfidf = model.named_steps[\"tfidf\"]\n",
    "lr = model.named_steps[\"lr\"]\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefs = lr.coef_[0]\n",
    "\n",
    "lr.classes_\n",
    "\n",
    "coefs.shape\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "top_real_idx = np.argsort(coefs)[:top_n]\n",
    "top_fake_idx = np.argsort(coefs)[-top_n:]\n",
    "\n",
    "top_real_words = feature_names[top_real_idx]\n",
    "top_fake_words = feature_names[top_fake_idx]\n",
    "\n",
    "top_real_words, top_fake_words\n",
    "\n",
    "fake_df = pd.DataFrame({\n",
    "    \"coef\": coefs[top_fake_idx],\n",
    "    \"word\": feature_names[top_fake_idx]\n",
    "}).sort_values(\"coef\", ascending=True)\n",
    "\n",
    "real_df = pd.DataFrame({\n",
    "    \"coef\": coefs[top_real_idx],\n",
    "    \"word\": feature_names[top_real_idx]\n",
    "}).sort_values(\"coef\", ascending=False)\n",
    "\n",
    "real_df, fake_df\n",
    "\n",
    "fake_df.head(10)\n",
    "real_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8d675-d212-41f3-82bd-ea288d403572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING TOP WORDS\n",
    "\n",
    "N = 15\n",
    "\n",
    "fake_plot = fake_df.head(N)[::-1]\n",
    "real_plot = real_df.head(N)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(fake_plot[\"word\"], fake_plot[\"coef\"], color=\"red\")\n",
    "plt.title(\"Top words pushing towards class 1\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(real_plot[\"word\"], real_plot[\"coef\"], color=\"blue\")\n",
    "plt.title(\"Top words pushing towards class 0\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d912fab-a1d7-487f-8643-9f03e4b5c4bd",
   "metadata": {},
   "source": [
    "3. LLM evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95681dc2-838e-46da-95f2-b6eeb9888895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE FROM X_test FOR LLM EVALUATION\n",
    "\n",
    "sample_size = 80\n",
    "\n",
    "sample = X_test.sample(sample_size, random_state=42)\n",
    "sample_labels = y_test.loc[sample.index]\n",
    "\n",
    "sample_size, sample_labels.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca94ea-9ff6-405e-a221-22676ab43cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. LLM Baseline — Zero-Shot Classification\n",
    "\n",
    "Unlike the ML model, LLMs were not trained on this dataset. We evaluate how well several GPT models perform in a zero-shot setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bd52f-75f6-4600-a4f2-98a6a52dd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM BASELINE: GPT-4o-mini\n",
    "\n",
    "def classify_with_llm_mini(text: str) ->int:\n",
    "    \"\"\" Calling the GPT-4.0-mini to classify the news:\n",
    "    0 = real\n",
    "    1 = fake\n",
    "    Returns int (1, 0). If the answer is not clearly 0 or 1, it defaults to 0.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an expert news fact-checker.\n",
    "    Task: Decide whether this news article is real (label 0) or fake (label 1).\n",
    "    Return only a single digit: 0 or 1.\n",
    "    No explanation needed.\n",
    "\n",
    "    Article: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "          response = client.chat.completions.create(\n",
    "              model=\"gpt-4o-mini\",\n",
    "              messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "              max_tokens=5, \n",
    "              temperature=0.0,\n",
    "          )\n",
    "          answer = response.choices[0].message.content.strip()\n",
    "          # A clear response\n",
    "          if answer in [\"0\", \"1\"]:\n",
    "            return int(answer)\n",
    "\n",
    "          #Idely: model answers with \"0\" or \"1\"\n",
    "          if \"1\" in answer:\n",
    "              return 1\n",
    "          else:\n",
    "              return 0\n",
    "\n",
    "    except Exception as e:\n",
    "          print(\"LLM error:\", e)\n",
    "          return 0\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019beffb-4c95-41f5-87a7-5badda66f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM PREDICTIONS: GPT-4o-mini\n",
    "\n",
    "llm_preds_mini = []\n",
    "for text in tqdm(sample, desc=\"LLM predicting\"):\n",
    "    llm_preds_mini.append(classify_with_llm_mini(text))\n",
    "\n",
    "llm_preds_mini = np.array(llm_preds_mini)\n",
    "len(llm_preds_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0a7ba-c165-4b9f-99e5-b7d1a6cebca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM Accuracy:\", accuracy_score(sample_labels, llm_preds_mini))\n",
    "print()\n",
    "print(\"LLM classification report:\")\n",
    "print(classification_report(sample_labels, llm_preds_mini))\n",
    "print()\n",
    "print(\"LLM confusion matrix:\")\n",
    "print(confusion_matrix(sample_labels, llm_preds_mini))\n",
    "\n",
    "ml_preds_sample = model.predict(sample)\n",
    "\n",
    "print()\n",
    "print(\"ML Accuracy (LogReg + TF-IDF):\", accuracy_score(sample_labels, ml_preds_sample))\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"text\": sample.values,\n",
    "    \"true_label\": sample_labels.values,\n",
    "    \"ml_pred\": ml_preds_sample,\n",
    "    \"llm_pred\": llm_preds_mini,\n",
    "})\n",
    "\n",
    "comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b899654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"LogReg\", \"GPT-4o-mini\", \"GPT-4o-mini+\", \"GPT-4.1-mini\", \"GPT-4o\"]\n",
    "accuracy = [0.9625, 0.65, 0.65, 0.2625, 0.3375]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(models, accuracy)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Comparison: ML vs LLM\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ml_cm = np.array([[27, 3],\n",
    "                  [2, 48]])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(ml_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix – Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_cm = np.array([[27, 25],\n",
    "                   [28, 21]])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(llm_cm, annot=True, fmt=\"d\", cmap=\"Purples\")\n",
    "plt.title(\"Confusion Matrix – GPT-4o (Zero-Shot)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf56eca-dccb-4272-bf32-d81627656490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LLM BASELINE: GPT-4o with extended prompt\n",
    "\n",
    "def classify_with_llm_v2(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Improved LLM classifier with detailed criteria.\n",
    "    0 = real\n",
    "    1 = fake\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a professional fake-news detection system.\n",
    "\n",
    "Your task: classify the following news article as REAL (0) or FAKE (1).\n",
    "\n",
    "Follow these guidelines:\n",
    "- If the article contains extreme claims, conspiracy, sensational language → more likely FAKE (1)\n",
    "- If the article describes normal events, facts, institutions, politics, people → more likely REAL (0)\n",
    "- If unsure, choose the class that fits BEST based on tone, structure, and content\n",
    "- Do NOT default to 0. Carefully consider both options.\n",
    "- Return ONLY a single digit: 0 or 1.\n",
    "\n",
    "Article:\n",
    "{text}\n",
    "\n",
    "Return only: 0 or 1\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0.0, \n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        if answer in [\"0\", \"1\"]:\n",
    "            return int(answer)\n",
    "\n",
    "        # If something weird returns\n",
    "        if \"1\" in answer:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLM error:\", e)\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31248784-474a-4e6a-90a2-e5b746167ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM PREDICTIONS: GPT-4o\n",
    "\n",
    "llm_preds_v2 = []\n",
    "for text in tqdm(sample, desc=\"LLM V2 predicting\"):\n",
    "    llm_preds_v2.append(classify_with_llm_v2(text))\n",
    "\n",
    "llm_preds_v2 = np.array(llm_preds_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37295c25-461d-4c2f-b72f-1e3268b31914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM V2 Accuracy:\", accuracy_score(sample_labels, llm_preds_v2))\n",
    "print()\n",
    "print(\"LLM V2 classification report:\")\n",
    "print(classification_report(sample_labels, llm_preds_v2))\n",
    "print()\n",
    "print(\"LLM V2 confusion matrix:\")\n",
    "print(confusion_matrix(sample_labels, llm_preds_v2))\n",
    "\n",
    "#Comparison sample\n",
    "comparison_v2 = pd.DataFrame({\n",
    "    \"text\": sample.values,\n",
    "    \"true_label\": sample_labels.values,\n",
    "    \"ml_pred\": model.predict(sample),\n",
    "    \"llm_pred_v2\": llm_preds_v2,\n",
    "})\n",
    "comparison_v2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab53c2-aa5f-4d54-84b6-444a616261a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LLM BASELINE: GPT-41-mini\n",
    "\n",
    "def classify_with_llm_v3(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Improved LLM classifier using gpt-4.1-mini.\n",
    "    Much better at classification tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You classify news articles.\n",
    "Return ONLY:\n",
    "0 = real\n",
    "1 = fake\n",
    "\n",
    "Criteria for FAKE:\n",
    "- sensational claims\n",
    "- conspiracy theory language\n",
    "- extreme emotional tone\n",
    "- zero factual grounding\n",
    "- fabricated or impossible events\n",
    "\n",
    "Criteria for REAL:\n",
    "- consistent with known political, social, and economic reality\n",
    "- journalistic tone\n",
    "- grounded in institutions, events, and people\n",
    "\n",
    "Be strict. Do NOT default to 0 when unsure.\n",
    "\n",
    "Article:\n",
    "{text}\n",
    "\n",
    "Return only 0 or 1.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        if answer in [\"0\", \"1\"]:\n",
    "            return int(answer)\n",
    "\n",
    "        # fallback\n",
    "        if \"1\" in answer:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "         print(\"LLM error:\", e)\n",
    "         return 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b01e7-abc0-4792-a260-3f505cd030a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_preds_v3 = [] \n",
    "\n",
    "for text in tqdm(sample, desc=\"LLM V3 (gpt-4.1-mini) predicting\"):\n",
    "    llm_preds_v3.append(classify_with_llm_v3(text))\n",
    "\n",
    "llm_preds_v3 = np.array(llm_preds_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f50c8-e0fd-42dd-803a-759b34cf1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM V3 Accuracy:\", accuracy_score(sample_labels, llm_preds_v3))\n",
    "print()\n",
    "print(\"LLM V3 classification report:\")\n",
    "print(classification_report(sample_labels, llm_preds_v3))\n",
    "print()\n",
    "print(\"LLM V3 confusion matrix:\")\n",
    "print(confusion_matrix(sample_labels, llm_preds_v3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c544e7-397f-4f28-99b1-ea752b0da9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LLM BASELINE: GPT-4o\n",
    "\n",
    "def classify_with_llm_v4( text: str) -> int:\n",
    "    prompt = f\"\"\"\n",
    "You are an expert fake news detector.\n",
    "Classify the article as REAL (0) or FAKE (1).\n",
    "Return ONLY a single digit 0 or 1.\n",
    "\n",
    "Article:\n",
    "{text}\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        if answer in [\"0\", \"1\"]:\n",
    "            return int(answer)\n",
    "        if \"1\" in answer:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLM error:\", e)\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cb1f7-bbb5-4939-9e78-bc3aee9780a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_preds_v4 = []\n",
    "for text in tqdm(sample, desc=\"LLM V4 (gpt-4o) predicting\"):\n",
    "    llm_preds_v4.append(classify_with_llm_v4(text))\n",
    "\n",
    "llm_preds_v4 = np.array(llm_preds_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696815fe-a995-41ae-9fd3-69c23827e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM V4 Accuracy:\", accuracy_score(sample_labels, llm_preds_v4))\n",
    "print(classification_report(sample_labels, llm_preds_v4))\n",
    "print(confusion_matrix(sample_labels, llm_preds_v4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
