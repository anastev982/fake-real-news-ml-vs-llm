


import sys
print(sys.executable)



# IMPORTS AND OPENAI CLIENT

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from tqdm import tqdm
import os
import openai

# expects that OPENAI_API_KEY is set in the environment, e.g. via .env or system env
#os.environ["OPENAI_API_KEY"] = "YOUR-API-KEY"
client = openai.OpenAI()


DATA_DIR = Path(".") / "data"
DATA_DIR, list(DATA_DIR.iterdir())


import sys
get_ipython().getoutput("{sys.executable} -m pip install seaborn matplotlib scikit-learn openai")






# LOAD DATA

df = pd.read_csv(DATA_DIR / "data.csv")

df.head()


# PREPROCESS TEXT

df["text"] = df["Headline"].fillna("") + " " + df["Body"].fillna("")
data = df[["text", "Label"]].rename(columns={"Label": "label"})
data = data.dropna(subset=["text", "label"])

data.head()

data["label"].value_counts()


# EDA

plt.figure(figsize=(6,4))
sns.countplot(x="label", data=data)
plt.title("Distribution of Real vs Fake News")
plt.xlabel("Label (0 = Real, 1 = Fake)")
plt.ylabel("Count")
plt.show()

data["text_length"] = data["text"].apply(lambda x: len(x.split()))
data["text_length"].describe()

# HISTOGRAM

plt.figure(figsize=(8,4))
sns.histplot(data["text_length"], bins=50)
plt.title("Distribution of Text Lengths")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

data.groupby("label")["text_length"].mean()





# TRAIN / TEST SPLIT

X = data["text"]
y = data["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train.shape, X_test.shape


# TF-IDF + LOGISTIC REGRESSION PIPELINE

model = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=50000,
        min_df=2,
        max_df=0.8,
        stop_words="english"
    )),
    ("lr", LogisticRegression(max_iter=200))
])
model.fit(X_train, y_train)


# EVALUATION OF ML MODEL

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print()
print(classification_report(y_test, y_pred))
print()
print(confusion_matrix(y_test, y_pred))


# FEATURE IMPORTANCE (TOP WORDS FOR EACH CLASS)

feature_names = model.named_steps["tfidf"].get_feature_names_out()

tfidf = model.named_steps["tfidf"]
lr = model.named_steps["lr"]

feature_names = tfidf.get_feature_names_out()
coefs = lr.coef_[0]

lr.classes_

coefs.shape

top_n = 20

top_real_idx = np.argsort(coefs)[:top_n]
top_fake_idx = np.argsort(coefs)[-top_n:]

top_real_words = feature_names[top_real_idx]
top_fake_words = feature_names[top_fake_idx]

top_real_words, top_fake_words

fake_df = pd.DataFrame({
    "coef": coefs[top_fake_idx],
    "word": feature_names[top_fake_idx]
}).sort_values("coef", ascending=True)

real_df = pd.DataFrame({
    "coef": coefs[top_real_idx],
    "word": feature_names[top_real_idx]
}).sort_values("coef", ascending=False)

real_df, fake_df

fake_df.head(10)
real_df.head(10)



# PLOTTING TOP WORDS

N = 15

fake_plot = fake_df.head(N)[::-1]
real_plot = real_df.head(N)[::-1]

plt.figure(figsize=(10, 6))
plt.barh(fake_plot["word"], fake_plot["coef"], color="red")
plt.title("Top words pushing towards class 1")
plt.xlabel("Coefficient")
plt.ylabel("Words")
plt.show()

plt.figure(figsize=(10, 6))
plt.barh(real_plot["word"], real_plot["coef"], color="blue")
plt.title("Top words pushing towards class 0")
plt.xlabel("Coefficient")
plt.ylabel("Words")
plt.show()





# SAMPLE FROM X_test FOR LLM EVALUATION

sample_size = 80

sample = X_test.sample(sample_size, random_state=42)
sample_labels = y_test.loc[sample.index]

sample_size, sample_labels.value_counts()



3. LLM Baseline — Zero-Shot Classification

Unlike the ML model, LLMs were not trained on this dataset. We evaluate how well several GPT models perform in a zero-shot setting.



# LLM BASELINE: GPT-4o-mini

def classify_with_llm_mini(text: str) ->int:
    """ Calling the GPT-4.0-mini to classify the news:
    0 = real
    1 = fake
    Returns int (1, 0). If the answer is not clearly 0 or 1, it defaults to 0."""

    prompt = f"""You are an expert news fact-checker.
    Task: Decide whether this news article is real (label 0) or fake (label 1).
    Return only a single digit: 0 or 1.
    No explanation needed.

    Article: 
    {text}
    """
    try:
          response = client.chat.completions.create(
              model="gpt-4o-mini",
              messages=[{"role": "user", "content": prompt}],
              max_tokens=5, 
              temperature=0.0,
          )
          answer = response.choices[0].message.content.strip()
          # A clear response
          if answer in ["0", "1"]:
            return int(answer)

          #Idely: model answers with "0" or "1"
          if "1" in answer:
              return 1
          else:
              return 0

    except Exception as e:
          print("LLM error:", e)
          return 0
   


# LLM PREDICTIONS: GPT-4o-mini

llm_preds_mini = []
for text in tqdm(sample, desc="LLM predicting"):
    llm_preds_mini.append(classify_with_llm_mini(text))

llm_preds_mini = np.array(llm_preds_mini)
len(llm_preds_mini)


print("LLM Accuracy:", accuracy_score(sample_labels, llm_preds_mini))
print()
print("LLM classification report:")
print(classification_report(sample_labels, llm_preds_mini))
print()
print("LLM confusion matrix:")
print(confusion_matrix(sample_labels, llm_preds_mini))

ml_preds_sample = model.predict(sample)

print()
print("ML Accuracy (LogReg + TF-IDF):", accuracy_score(sample_labels, ml_preds_sample))

comparison = pd.DataFrame({
    "text": sample.values,
    "true_label": sample_labels.values,
    "ml_pred": ml_preds_sample,
    "llm_pred": llm_preds_mini,
})

comparison.head()


import matplotlib.pyplot as plt

models = ["LogReg", "GPT-4o-mini", "GPT-4o-mini+", "GPT-4.1-mini", "GPT-4o"]
accuracy = [0.9625, 0.65, 0.65, 0.2625, 0.3375]

plt.figure(figsize=(8,5))
plt.bar(models, accuracy)
plt.ylabel("Accuracy")
plt.title("Accuracy Comparison: ML vs LLM")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()



import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

ml_cm = np.array([[27, 3],
                  [2, 48]])

plt.figure(figsize=(5,4))
sns.heatmap(ml_cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix – Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()



llm_cm = np.array([[27, 25],
                   [28, 21]])

plt.figure(figsize=(5,4))
sns.heatmap(llm_cm, annot=True, fmt="d", cmap="Purples")
plt.title("Confusion Matrix – GPT-4o (Zero-Shot)")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()



#  LLM BASELINE: GPT-4o with extended prompt

def classify_with_llm_v2(text: str) -> int:
    """
    Improved LLM classifier with detailed criteria.
    0 = real
    1 = fake
    """

    prompt = f"""
You are a professional fake-news detection system.

Your task: classify the following news article as REAL (0) or FAKE (1).

Follow these guidelines:
- If the article contains extreme claims, conspiracy, sensational language → more likely FAKE (1)
- If the article describes normal events, facts, institutions, politics, people → more likely REAL (0)
- If unsure, choose the class that fits BEST based on tone, structure, and content
- Do NOT default to 0. Carefully consider both options.
- Return ONLY a single digit: 0 or 1.

Article:
{text}

Return only: 0 or 1
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=5,
            temperature=0.0, 
        )

        answer = response.choices[0].message.content.strip()

        if answer in ["0", "1"]:
            return int(answer)

        # If something weird returns
        if "1" in answer:
            return 1
        return 0

    except Exception as e:
        print("LLM error:", e)
        return 0
        


# LLM PREDICTIONS: GPT-4o

llm_preds_v2 = []
for text in tqdm(sample, desc="LLM V2 predicting"):
    llm_preds_v2.append(classify_with_llm_v2(text))

llm_preds_v2 = np.array(llm_preds_v2)


print("LLM V2 Accuracy:", accuracy_score(sample_labels, llm_preds_v2))
print()
print("LLM V2 classification report:")
print(classification_report(sample_labels, llm_preds_v2))
print()
print("LLM V2 confusion matrix:")
print(confusion_matrix(sample_labels, llm_preds_v2))

#Comparison sample
comparison_v2 = pd.DataFrame({
    "text": sample.values,
    "true_label": sample_labels.values,
    "ml_pred": model.predict(sample),
    "llm_pred_v2": llm_preds_v2,
})
comparison_v2.head()



#  LLM BASELINE: GPT-41-mini

def classify_with_llm_v3(text: str) -> int:
    """
    Improved LLM classifier using gpt-4.1-mini.
    Much better at classification tasks.
    """

    prompt = f"""
You classify news articles.
Return ONLY:
0 = real
1 = fake

Criteria for FAKE:
- sensational claims
- conspiracy theory language
- extreme emotional tone
- zero factual grounding
- fabricated or impossible events

Criteria for REAL:
- consistent with known political, social, and economic reality
- journalistic tone
- grounded in institutions, events, and people

Be strict. Do NOT default to 0 when unsure.

Article:
{text}

Return only 0 or 1.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=5,
            temperature=0.0,
        )
        answer = response.choices[0].message.content.strip()
        
        if answer in ["0", "1"]:
            return int(answer)

        # fallback
        if "1" in answer:
            return 1
        return 0

    except Exception as e:
         print("LLM error:", e)
         return 0


    


llm_preds_v3 = [] 

for text in tqdm(sample, desc="LLM V3 (gpt-4.1-mini) predicting"):
    llm_preds_v3.append(classify_with_llm_v3(text))

llm_preds_v3 = np.array(llm_preds_v3)



print("LLM V3 Accuracy:", accuracy_score(sample_labels, llm_preds_v3))
print()
print("LLM V3 classification report:")
print(classification_report(sample_labels, llm_preds_v3))
print()
print("LLM V3 confusion matrix:")
print(confusion_matrix(sample_labels, llm_preds_v3))



#  LLM BASELINE: GPT-4o

def classify_with_llm_v4( text: str) -> int:
    prompt = f"""
You are an expert fake news detector.
Classify the article as REAL (0) or FAKE (1).
Return ONLY a single digit 0 or 1.

Article:
{text}"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=5,
            temperature=0.0,
        )

        answer = response.choices[0].message.content.strip()

        if answer in ["0", "1"]:
            return int(answer)
        if "1" in answer:
            return 1
        return 0

    except Exception as e:
        print("LLM error:", e)
        return 0
    
    


llm_preds_v4 = []
for text in tqdm(sample, desc="LLM V4 (gpt-4o) predicting"):
    llm_preds_v4.append(classify_with_llm_v4(text))

llm_preds_v4 = np.array(llm_preds_v4)


print("LLM V4 Accuracy:", accuracy_score(sample_labels, llm_preds_v4))
print(classification_report(sample_labels, llm_preds_v4))
print(confusion_matrix(sample_labels, llm_preds_v4))









